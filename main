{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8706391,"sourceType":"datasetVersion","datasetId":5222373},{"sourceId":8775105,"sourceType":"datasetVersion","datasetId":5273960}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Suspend printing to stdout\nimport sys\nimport os\noriginal_stdout = sys.stdout\n\n# Redirect stdout to devnull\nsys.stdout = open(os.devnull, 'w')\n\n# !pip install chromadb\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# from langchain.llms import HuggingFaceHub\nimport os\n# from getpass import getpass\n# from langchain.llms.huggingface_pipeline import HuggingFacePipeline\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline,  Trainer, TrainingArguments, AdamW\nfrom transformers import GPTNeoForCausalLM, GPT2Tokenizer\n# from chromadb import ChromaClient\n# import chromadb\nimport nltk\nimport re\n# import spacy\n# import json\nfrom tqdm import tqdm\n# import torch.multiprocessing as mp\n# from torch.utils.data.distributed import DistributedSampler\n# from torch.nn.parallel import DistributedDataParallel as DDP\n# from torch.distributed import init_process_group, destroy_process_group\n# from pathlib import Path\n# import datetime\nfrom peft import get_peft_config, PeftType, LoraConfig ,get_peft_model,PeftModel, PeftConfig\n# from transformers import BitsAndBytesConfig\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nworld_size = torch.cuda.device_count()\n# mp.spawn(<selfcontainedmethodforeachproc>, nprocs=world_size, args=(args,))\n# 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  Huggingface access token\n# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"HF Token:\")\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\nos.environ[\"ALLOW_RESET\"] = \"TRUE\"\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'garbage_collection_threshold:0.6,max_split_size_mb:128'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # Ensuring both GPUs are visible to PyTorch\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nsys.stdout = original_stdout  #Resume printing ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-27T09:43:00.208513Z","iopub.execute_input":"2024-06-27T09:43:00.209219Z","iopub.status.idle":"2024-06-27T09:43:01.501595Z","shell.execute_reply.started":"2024-06-27T09:43:00.209182Z","shell.execute_reply":"2024-06-27T09:43:01.500894Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:42:46.580588Z","iopub.execute_input":"2024-06-27T09:42:46.580980Z","iopub.status.idle":"2024-06-27T09:43:00.206109Z","shell.execute_reply.started":"2024-06-27T09:42:46.580949Z","shell.execute_reply":"2024-06-27T09:43:00.205208Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_id = \"EleutherAI/gpt-neo-2.7B\"\n# model_id = \"EleutherAI/gpt-neo-125m\"\nmodel_id = \"EleutherAI/gpt-neo-1.3B\"\n# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n#DistilGPT-2 , T5-Small , GPT-2 small","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:43:01.503004Z","iopub.execute_input":"2024-06-27T09:43:01.503305Z","iopub.status.idle":"2024-06-27T09:45:23.498226Z","shell.execute_reply.started":"2024-06-27T09:43:01.503281Z","shell.execute_reply":"2024-06-27T09:45:23.497599Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18d343b12cd34ee186fcfeac7caa5ade"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1757e044d4f4a3b921983bb93a05b76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"781d980a6a0c49a2b3732d9dd3a3902f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7e33179a7b0486484515edb94d6af66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a954ae3ce6543fc8d92eb0f6360ea79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78c126645031446d9feaf77de6b0dd1e"}},"metadata":{}}]},{"cell_type":"code","source":"peft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    inference_mode=False,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0.1,\n#     target_modules=['q', 'v'], \n#     target_modules=[\"query_key_value\"],\n#     target_modules=[\"transformer.h\"],\n    bias=\"none\",\n)\n# model.add_adapter(peft_config)\npeft_model = get_peft_model(model, peft_config)\npeft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:45:23.500970Z","iopub.execute_input":"2024-06-27T09:45:23.501380Z","iopub.status.idle":"2024-06-27T09:45:23.656091Z","shell.execute_reply.started":"2024-06-27T09:45:23.501348Z","shell.execute_reply":"2024-06-27T09:45:23.655491Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/textdata/data0.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n    \n# client = chromadb.Client()\n# client.reset()\n# client = chromadb.PersistentClient(path=\"/kaggle/working/chromadb\")\n# collection = client.get_collection(name=\"llm_lec_text\")\n\n# Create a collection for the LLM lectures\n# collection = client.create_collection(name=\"llm_lec_text\")\n# client.heartbeat() # returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n# client.reset() # Empties and completely resets the database. ⚠️ This is destructive and not reversible.","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:45:23.657301Z","iopub.execute_input":"2024-06-27T09:45:23.657638Z","iopub.status.idle":"2024-06-27T09:45:24.400823Z","shell.execute_reply.started":"2024-06-27T09:45:23.657596Z","shell.execute_reply":"2024-06-27T09:45:24.400179Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')\ntokenizer.pad_token = tokenizer.eos_token\n\ncleaned_text = re.sub(r'[\\\\\\[\\]{}@]', '', text)\ncleaned_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', cleaned_text)\ncleaned_text = cleaned_text.lower().strip()\n\ntokens = tokenizer.tokenize(cleaned_text)\n\n# 5. Chunk text\nmax_length = 96\nchunks = []\ncurrent_chunk = []\nfor token in tokens:\n    current_chunk.append(token)\n    if len(current_chunk) >= max_length:\n        chunks.append(current_chunk)\n        current_chunk = []\n\n# Add the last chunk\nif current_chunk:\n    chunks.append(current_chunk)\n\n# tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n# tokenizer.pad_token = tokenizer.eos_token\nips = []\nfor chunk in chunks:\n    ips.append(tokenizer(chunk, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length'))\n    \nencodings = {\n    'input_ids': [],\n    'attention_mask': []\n}\n\nfor ip in ips:\n    encodings['input_ids'].append(ip['input_ids'][0])\n    encodings['attention_mask'].append(ip['attention_mask'][0])\n\n# Convert lists to tensors\nencodings = {key: torch.stack(val) for key, val in encodings.items()}\n\n# class TextDataset(Dataset):\n#     def __init__(self, encodings):\n#         self.encodings = encodings\n\n#     def __len__(self):\n#         return len(self.encodings['input_ids'])\n\n#     def __getitem__(self, idx):\n#         it = {key: tensor[idx] for key, tensor in self.encodings.items()}\n#         # Shift input_ids by one to the right for labels\n#         it['label'] = it['input_ids'].clone()\n#         it['label'][1:] = it['input_ids'][:-1]\n#         it['label'][0] = -100  # Mask the first token\n#         return it\n\n\nclass TextDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n    def __getitem__(self, idx):\n        it = {key: tensor[idx] for key, tensor in self.encodings.items()}\n        it['labels'] = it['input_ids'].clone()\n        it['labels'][:-1] = it['input_ids'][1:]\n        it['labels'][-1] = -100  # Mask the first token\n        return it\n# dataT = torch.tensor(data.values)\n# dataC = dataT.reshape(4292,24,3)\n# dataset = torch.utils.data.Dataset(encodings)\ndataset = TextDataset(encodings)\ndataloader = DataLoader(dataset,\n                        batch_size=32, \n                        shuffle=True,\n#                         num_workers=2,\n#                        sampler=distDS,\n#                         drop_last=False,\n#                        pin_memory=True\n                       )","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:45:24.402140Z","iopub.execute_input":"2024-06-27T09:45:24.402409Z","iopub.status.idle":"2024-06-27T09:45:37.989193Z","shell.execute_reply.started":"2024-06-27T09:45:24.402386Z","shell.execute_reply":"2024-06-27T09:45:37.988572Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# data = pd.read_csv('/kaggle/input/emb-data/EmbData(1).csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    overwrite_output_dir=True,\n    num_train_epochs=7,\n    per_device_train_batch_size=4,\n    warmup_steps=500,\n#     weight_decay=0.01,\n    save_steps=10_000,\n    save_total_limit=5,\n    logging_steps=200,\n    learning_rate=1e-4,\n    weight_decay=0.01,\n)\n\n# Initialize the PeftTrainer\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=dataset,\n)\n\n# Train the model\ntrainer.train()\n# 15c56e4ed81a3f19189050629366c3e7d58d141c wandb api key","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:45:37.990542Z","iopub.execute_input":"2024-06-27T09:45:37.990803Z","iopub.status.idle":"2024-06-27T09:59:00.226326Z","shell.execute_reply.started":"2024-06-27T09:45:37.990780Z","shell.execute_reply":"2024-06-27T09:59:00.225649Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240627_094648-69qt1tbx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/vitp/huggingface/runs/69qt1tbx' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/vitp/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/vitp/huggingface' target=\"_blank\">https://wandb.ai/vitp/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/vitp/huggingface/runs/69qt1tbx' target=\"_blank\">https://wandb.ai/vitp/huggingface/runs/69qt1tbx</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1939' max='1939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1939/1939 11:53, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>4.071100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.109700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.070000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.063900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.060500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.059700</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.060200</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.057100</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.055800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1939, training_loss=0.4792808733876677, metrics={'train_runtime': 799.6712, 'train_samples_per_second': 9.699, 'train_steps_per_second': 2.425, 'total_flos': 5426827093868544.0, 'train_loss': 0.4792808733876677, 'epoch': 7.0})"},"metadata":{}}]},{"cell_type":"code","source":"peft_model.save_pretrained(\"./finalFT\")","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:59:00.227441Z","iopub.execute_input":"2024-06-27T09:59:00.227701Z","iopub.status.idle":"2024-06-27T09:59:00.753887Z","shell.execute_reply.started":"2024-06-27T09:59:00.227678Z","shell.execute_reply":"2024-06-27T09:59:00.753076Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_id = \"EleutherAI/gpt-neo-125m\"\n# tokenizer = AutoTokenizer.from_pretrained(model_id)\npeft_model_path = './finalFT'\nmodel = PeftModel.from_pretrained(model, peft_model_path).to(device)\n# modelOG = AutoModelForCausalLM.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:59:37.391013Z","iopub.execute_input":"2024-06-27T09:59:37.391865Z","iopub.status.idle":"2024-06-27T09:59:37.573217Z","shell.execute_reply.started":"2024-06-27T09:59:37.391817Z","shell.execute_reply":"2024-06-27T09:59:37.572356Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def generate_answer(question, max_length=100):\n    input_text = f\"Question: {question}\\nAnswer:\"\n    tokenizer.pad_token = tokenizer.eos_token\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n    return answer.split(\"Answer:\")[-1].strip()\n\ngenerate_answer(input())","metadata":{"execution":{"iopub.status.busy":"2024-06-27T10:10:40.124263Z","iopub.execute_input":"2024-06-27T10:10:40.124961Z","iopub.status.idle":"2024-06-27T10:10:48.109369Z","shell.execute_reply.started":"2024-06-27T10:10:40.124927Z","shell.execute_reply":"2024-06-27T10:10:48.108600Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdin","text":" What is CS324 ?\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'A computer science course that is taught by a group of students from the University of Cambridge.\\n\\nQuestion\\nWhat is the difference between a computer scientist and a programmer? \\nA:'"},"metadata":{}}]},{"cell_type":"code","source":"model_id = \"EleutherAI/gpt-neo-1.3B\"\nmodelOG = AutoModelForCausalLM.from_pretrained(model_id).to(device)\ndef generate_answer(question, max_length=100):\n    input_text = f\"Question: {question}\\nAnswer:\"\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n    output = modelOG.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n    return answer.split(\"Answer:\")[-1].strip()\ngenerate_answer(input())","metadata":{"execution":{"iopub.status.busy":"2024-06-27T10:10:51.111931Z","iopub.execute_input":"2024-06-27T10:10:51.112731Z","iopub.status.idle":"2024-06-27T10:11:04.933193Z","shell.execute_reply.started":"2024-06-27T10:10:51.112700Z","shell.execute_reply":"2024-06-27T10:11:04.932434Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdin","text":" What is CS324 ?\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'CS is the Computer Science department of the University of California, Berkeley.\\n\\nQuestion\\nWhat is a CS student?\\nAnswers:\\nA student is someone who is enrolled in a course of study at a university. A student may be a student in the first year of a degree program, a graduate student, or a post-doctoral fellow. Students may also be enrolled as part of an undergraduate or graduate program. The term \"student\"'"},"metadata":{}}]},{"cell_type":"code","source":"MODEL_PATH = Path('/kaggle/working/models')\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\nMODEL_NAME = 'gptNeoFT.pt'\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:27:12.502132Z","iopub.execute_input":"2024-06-26T09:27:12.503072Z","iopub.status.idle":"2024-06-26T09:27:12.507521Z","shell.execute_reply.started":"2024-06-26T09:27:12.503038Z","shell.execute_reply":"2024-06-26T09:27:12.506620Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model_id = \"EleutherAI/gpt-neo-1.3B\"\n# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\ntokenizer1 = AutoTokenizer.from_pretrained(model_id)\nmodel1 = AutoModelForCausalLM.from_pretrained(model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Freeze the first 12 transformer layers\nfor param in model.transformer.h[:16].parameters():\n    param.requires_grad = False\n\n# Move the model to the device and set to bfloat16\nmodel = model.to(device).to(torch.bfloat16)\nmodel.train()\n\n# Define optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=7e-3)\n\n# Training loop\nfor epoch in range(101):\n    loop = tqdm(dataloader, leave=True)\n    for step, x in enumerate(loop):\n        input_ids = x[0][:,:, 0].to(device)\n        attention_mask = x[0][:,:, 1].to(device)\n        labels = x[0][:,:, 0].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())\n\n        torch.cuda.empty_cache()\n    if epoch%10 == 0:\n    # Save the model periodically\n        torch.save(model.state_dict(), f'{MODEL_PATH}/model_epoch_{epoch}.pt')\n        print(f'Model saved at epoch {epoch} ({datetime.datetime.now().timestamp()})')\n\nprint(\"Training complete.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:27:24.212987Z","iopub.execute_input":"2024-06-26T09:27:24.213722Z","iopub.status.idle":"2024-06-26T09:27:32.250210Z","shell.execute_reply.started":"2024-06-26T09:27:24.213688Z","shell.execute_reply":"2024-06-26T09:27:32.248764Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Epoch 0:  33%|███▎      | 1/3 [00:05<00:11,  5.59s/it, loss=7.06]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m][:,:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m labels \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m][:,:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:986\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    971\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m    972\u001b[0m     input_ids,\n\u001b[1;32m    973\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    982\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    983\u001b[0m )\n\u001b[1;32m    984\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 986\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 196.12 MiB is free. Process 2643 has 15.70 GiB memory in use. Of the allocated memory 14.88 GiB is allocated by PyTorch, and 538.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 196.12 MiB is free. Process 2643 has 15.70 GiB memory in use. Of the allocated memory 14.88 GiB is allocated by PyTorch, and 538.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"model = model.to(device).to(torch.bfloat16)\n# model = torch.nn.DataParallel(model, device_ids=[0, 1])\nmodel.train()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n\n# clip_value = 1.0\n# max_batches = 1\n# torch.save(obj=model.state_dict(),f=MODEL_SAVE_PATH)\n\nfor epoch in range(100):\n    loop = tqdm(dataloader, leave=True)\n    for step, (x, y) in enumerate(loop):\n#         if step >= max_batches:\n#             break\n        input_ids = x[:, 0].unsqueeze(dim=0).to(device)\n        attention_mask = x[:, 1].unsqueeze(dim=0).to(device)\n        labels = y.unsqueeze(dim=0).to(device)\n        \n#         print(f\"input_ids shape: {input_ids.shape}\")\n#         print(f\"attention_mask shape: {attention_mask.shape}\")\n#         print(f\"labels shape: {labels.shape}\")\n        \n#         if input_ids.size(0) != labels.size(0):\n#             min_batch_size = min(input_ids.size(0), labels.size(0))\n#             input_ids = input_ids[:min_batch_size]\n#             attention_mask = attention_mask[:min_batch_size]\n#             labels = labels[:min_batch_size]\n        \n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss.mean()\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n        \n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())\n        \n#         del input_ids, attention_mask, labels, outputs, loss\n        torch.cuda.empty_cache()\n#     if epoch%10 == 0:\n    torch.save(obj=model.state_dict(),f=MODEL_SAVE_PATH)\n    print(f'Model saved {datetime.datetime.now().timestamp()}')\n\nprint(\"Training complete.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm /kaggle/working/models/*","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:15:09.526144Z","iopub.execute_input":"2024-06-26T09:15:09.526809Z","iopub.status.idle":"2024-06-26T09:15:12.328120Z","shell.execute_reply.started":"2024-06-26T09:15:09.526777Z","shell.execute_reply":"2024-06-26T09:15:12.326986Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\ndownload_file('/kaggle/working/finalFT', 'gptNeo13_lora2')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T10:12:50.070232Z","iopub.execute_input":"2024-06-27T10:12:50.070977Z","iopub.status.idle":"2024-06-27T10:12:51.432768Z","shell.execute_reply.started":"2024-06-27T10:12:50.070944Z","shell.execute_reply":"2024-06-27T10:12:51.432023Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/gptNeo13_lora2.zip","text/html":"<a href='gptNeo13_lora2.zip' target='_blank'>gptNeo13_lora2.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"def clean_text(text):\n    # Remove unwanted characters and symbols\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    text = re.sub(r'\\.\\s*', '. ', text)  # Ensure periods are followed by a single space\n    text = re.sub(r'\\s*\\.\\s*', '. ', text)  # Remove spaces before periods\n    return text.strip()\n\n# Function to tokenize sentences using SpaCy\ndef spacyST(text):\n    nlp = spacy.load('en_core_web_sm')\n    doc = nlp(clean_text(text))\n    return [sent.text for sent in doc.sents]\n\ndef preproTxt(text):\n    sentText = spacyST(text)\n    tokenizer.pad_token = tokenizer.eos_token\n    emb = tokenizer(sentText,padding=True,truncation=True, return_tensors='pt')\n    return emb","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:22:45.460138Z","iopub.execute_input":"2024-06-26T09:22:45.460852Z","iopub.status.idle":"2024-06-26T09:22:45.467695Z","shell.execute_reply.started":"2024-06-26T09:22:45.460821Z","shell.execute_reply":"2024-06-26T09:22:45.466628Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# model = GPTNeoForCausalLM.from_pretrained('/kaggle/working/models/model_epoch_2.pt')\n# tokenizer = GPT2Tokenizer.from_pretrained('/kaggle/working/models/model_epoch_2.pt')\n\nmodel.to(device).eval()\nip = preproTxt(input())\nip.to(device)\ntokenizer.pad_token = tokenizer.eos_token\nwith torch.no_grad():\n    output_ids = model.generate(\n        input_ids=ip['input_ids'],\n        attention_mask=ip['attention_mask'],\n        max_length=500,  # Adjust the max_length as needed\n#         num_return_sequences=1,\n#         no_repeat_ngram_size=2,  # Optional: to reduce repetition\n        temperature=0.7,  # Optional: controls the creativity\n        top_k=50,  # Optional: top-k sampling\n        top_p=0.95,  # Optional: top-p sampling (nucleus sampling)\n        do_sample=True  # Set to True for sampling, False for greedy decoding\n    )\n\n# Decode the output\ngenerated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:22:50.560386Z","iopub.execute_input":"2024-06-26T09:22:50.561203Z","iopub.status.idle":"2024-06-26T09:22:57.052576Z","shell.execute_reply.started":"2024-06-26T09:22:50.561157Z","shell.execute_reply":"2024-06-26T09:22:57.051579Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdin","text":" Explain LLMs\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Explain LLMs\n","output_type":"stream"}]},{"cell_type":"code","source":"# model = GPTNeoForCausalLM.from_pretrained('/kaggle/working/models/model_epoch_2.pt')\n# tokenizer = GPT2Tokenizer.from_pretrained('/kaggle/working/models/model_epoch_2.pt')\n\nmodelOG.to(device).eval()\nip = preproTxt(input())\nip.to(device)\ntokenizer.pad_token = tokenizer.eos_token\nwith torch.no_grad():\n    output_ids = modelOG.generate(\n        input_ids=ip['input_ids'],\n        attention_mask=ip['attention_mask'],\n        max_length=500,  # Adjust the max_length as needed\n#         num_return_sequences=1,\n#         no_repeat_ngram_size=2,  # Optional: to reduce repetition\n        temperature=0.7,  # Optional: controls the creativity\n        top_k=50,  # Optional: top-k sampling\n        top_p=0.95,  # Optional: top-p sampling (nucleus sampling)\n        do_sample=True  # Set to True for sampling, False for greedy decoding\n    )\n\n# Decode the output\ngenerated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = model.half()\nmodel = model.to(torch.bfloat16).to(device)\n# model.gradient_checkpointing_enable()\nmodel = torch.nn.DataParallel(model, device_ids=[0, 1])\nmodel.train()\n# scaler = torch.cuda.amp.GradScaler()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n\nclip_value = 1.0\nmax_batches = 1\n# max_length = 512  \n# accumulation_steps = 64\n\n\nfor epoch in range(10):  # number of epochs\n    loop = tqdm(dataloader, leave=True)\n    for step, (x, y) in enumerate(loop):\n        if step >= max_batches:\n            break\n        input_ids = x[:, 0].to(device)  # Ensure the shape is (batch_size, sequence_length)\n        attention_mask = x[:, 1].to(device)  # Ensure the shape is (batch_size, sequence_length)\n        labels = y.to(device)  # Ensure the shape is (batch_size, sequence_length)\n#         with torch.cuda.amp.autocast():\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n    #         loss = outputs.loss / accumulation_steps\n        loss = outputs.loss.mean()\n#         loss = loss.mean()\n#         scaler.scale(loss).backward()\n        optimizer.zero_grad()\n        loss.backward()\n#         if (step + 1) % accumulation_steps == 0:\n#         scaler.unscale_(optimizer)\n        optimizer.step()\n#         scaler.step(optimizer)\n#         scaler.update()\n#         optimizer.zero_grad()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())\n#         loss.detach()\n        del input_ids, attention_mask, labels, outputs, loss\n        torch.cuda.empty_cache()\nprint(\"Training complete.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoded_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n\n# Create Dataset and DataLoader\nclass TextDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n    def __getitem__(self, idx):\n        it = {key: tensor[idx] for key, tensor in self.encodings.items()}\n        # Shift input_ids by one to the right for labels\n        it['label'] = it['input_ids'].clone()\n        it['label'][1:] = it['input_ids'][:-1]\n        it['label'][0] = -100  # Mask the first token\n        return it\n    \n# def prepare(rank):\ndataset = TextDataset(emb)\n# distDS = DistributedSampler(dataset=dataset, num_replicas =2, rank=rank, seed=42)\n# dataloader = DataLoader(dataset,\n#                         batch_size=batch_size, \n#                         shuffle=True,\n#                         num_workers=2,\n# #                        sampler=distDS,\n# #                         drop_last=False,\n# #                        pin_memory=True\n#                        )\n#     return dataloader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deepspeed=\"ds_config.json\",","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"{\n  \"zero_optimization\": {\n    \"stage\": 3,\n    \"offload_optimizer\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true\n    },\n    \"offload_param\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true\n    },\n    \"overlap_comm\": true,\n    \"contiguous_gradients\": true,\n    \"sub_group_size\": 1e9,\n    \"reduce_bucket_size\": \"auto\",\n    \"stage3_prefetch_bucket_size\": \"auto\",\n    \"stage3_param_persistence_threshold\": \"auto\",\n    \"stage3_max_live_parameters\": 1e9,\n    \"stage3_max_reuse_distance\": 1e9,\n    \"stage3_gather_fp16_weights_on_model_save\": true\n  }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.half()\n# model = model.to(torch.int8)\n# model = torch.nn.DataParallel(model, device_ids=[0, 1])\n\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/output',         # output directory\n    num_train_epochs=2,             # number of training epochs\n    per_device_train_batch_size=1,  # batch size for training\n    per_device_eval_batch_size=1,   # batch size for evaluation\n    warmup_steps=50,               # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,              # strength of weight decay\n    logging_dir='/kaggle/working/logs',           # directory for storing logs\n    logging_steps=10,\n    gradient_accumulation_steps=1,               # accumulate gradients over 4 steps\n    fp16=True,                                   # enable mixed precision training\n    gradient_checkpointing=True,\n    \n#     offload_state_dict=True,\n#     offload_optimizer=True,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=dataset,               # training dataset\n    tokenizer=tokenizer,                 # tokenizer\n#     use_cache=False,\n#     use_reentrant=False,\n)\n\n# Start training\ntrainer.train()\n\n# 15c56e4ed81a3f19189050629366c3e7d58d141c wandb api key","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sagemaker \n!pip install boto3\n\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFace\n\ntry:\n\trole = sagemaker.get_execution_role()\nexcept ValueError:\n\tiam = boto3.client('iam')\n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\t\t\nhyperparameters = {\n\t'model_name_or_path':'EleutherAI/gpt-neo-2.7B',\n\t'output_dir':'/opt/ml/model'\n\t# add your remaining hyperparameters\n\t# more info here https://github.com/huggingface/transformers/tree/v4.37.0/examples/pytorch/question-answering\n}\n\n# git configuration to download our fine-tuning script\ngit_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.37.0'}\n\n# creates Hugging Face estimator\nhuggingface_estimator = HuggingFace(\n\tentry_point='run_qa.py',\n\tsource_dir='./examples/pytorch/question-answering',\n\tinstance_type='ml.p3.2xlarge',\n\tinstance_count=1,\n\trole=role,\n\tgit_config=git_config,\n\ttransformers_version='4.37.0',\n\tpytorch_version='2.1.0',\n\tpy_version='py310',\n\thyperparameters = hyperparameters\n)\n\n# starting the train job\nhuggingface_estimator.fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ddp_setup(rank: int, world_size: int):\n    \"\"\"\n    Args:\n    rank: Unique identifier of each process\n    world_size: Total number of processes\n    \"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n#     torch.cuda.set_device(rank)\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    destroy_process_group()\n\n\ndef main(rank, world_size, model):\n        ddp_setup(rank,world_size)\n        dataloader = prepare(rank)\n        model = model.half()\n        model = model.cuda(rank)\n        model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)\n        model.train()\n        clip_value = 1.0\n        # Optimizer\n        optimizer = AdamW(model.parameters(), lr=5e-5)\n        accumulation_steps=16\n        print(\"Setup Complete\")\n        # Training loop\n        for epoch in range(10):  # number of epochs\n            dataloader.sampler.set_epoch(epoch)\n            loop = tqdm(dataloader, leave=True)\n            print(f\"Epoch Started : {epoch}\")\n            for step,batch in enumerate(loop):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n    #             input_ids = \n    #             attention_mask = \n    #             labels = \n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        #         loss = outputs.loss.mean()\n                loss = outputs.loss / accumulation_steps\n        #         print(f'Loss: {loss}')\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n                if (step+1)%accumulation_steps == 0:\n                    optimizer.step()\n                    optimizer.zero_grad()\n                loop.set_description(f'Epoch {epoch}')\n                loop.set_postfix(loss=loss.item())\n                torch.cuda.empty_cache()\n                print(f\"Epoch Ended : {epoch}\")\n        cleanup()\n        print(\"Training complete.\")\n        return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_size = 32  # adjust as necessary\n# emb = None  # Initialize your embedding object\n\n# Determine number of GPUs available\nworld_size = torch.cuda.device_count()\n\nfor rank in range(world_size):\n    print(f\"Rank: {rank}\")\n    trained = main(rank, world_size, model)\n\nprint(\"Training complete.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, DistributedSampler\nfrom tqdm import tqdm\nfrom transformers import AdamW\n\nif __name__ == '__main__':\n    # Kaggle environment setup\n    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n#     world_size = 2\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    world_size = torch.cuda.device_count()\n    \n\n    def prepare(rank):\n        dataset = TextDataset(emb)\n        distDS = DistributedSampler(dataset=dataset, num_replicas=2, rank=rank, seed=42)\n        dataloader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=2,\n            sampler=distDS,\n            drop_last=False,\n            pin_memory=True,\n        )\n        return dataloader\n\n    def ddp_setup(rank: int, world_size: int):\n        \"\"\"\n        Args:\n        rank: Unique identifier of each process\n        world_size: Total number of processes\n        \"\"\"\n        torch.distributed.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n\n    def cleanup():\n        dist.destroy_process_group()\n\n    def main(rank, world_size):\n        ddp_setup(rank, world_size)\n        dataloader = prepare(rank)\n        model = model.half()\n        model = model.cuda(rank)\n        model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)\n        model.train()\n        clip_value = 1.0\n        # Optimizer\n        optimizer = AdamW(model.parameters(), lr=5e-5)\n        accumulation_steps = 16\n\n        # Training loop\n        for epoch in range(10):  # number of epochs\n            dataloader.sampler.set_epoch(epoch)\n            loop = tqdm(dataloader, leave=True)\n            for step, batch in enumerate(loop):\n                input_ids = batch['input_ids'].cuda(rank, non_blocking=True)\n                attention_mask = batch['attention_mask'].cuda(rank, non_blocking=True)\n                labels = batch['labels'].cuda(rank, non_blocking=True)\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss / accumulation_steps\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n                if (step + 1) % accumulation_steps == 0:\n                    optimizer.step()\n                    optimizer.zero_grad()\n                loop.set_description(f'Epoch {epoch}')\n                loop.set_postfix(loss=loss.item())\n                torch.cuda.empty_cache()\n        cleanup()\n        print(\"Training complete.\")\n\n    mp.spawn(main, args=(world_size,), nprocs=world_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def getMax(Y):\n#   max = 0\n#   for i in Y:\n#     if max < len(i):\n#       max = len(i)\n#   return max\n\n# def padSeq(L, l, c):\n#   L1 = L[:]\n#   diff = l - len(L)\n#   if diff != 0:\n#     for i in range(diff):\n#       L1.append(c)\n#   return L1\n\n\n# #convert list to tensor\n# #pad y to max length present in the batch and convert to tensor , padd using the <End> token\n\n# def get_batch(X_train,y_train, X_test,y_test, batch_size,split=True):\n#   if split:\n#     dataX,dataY = X_train,y_train\n#   else:\n#    dataX, dataY = X_test,y_test\n#   ix = torch.randint(0,len(dataX),(batch_size,))\n#   x = torch.tensor([dataX[i] for i in ix]).to(device)\n#   y = [dataY[i] for i in ix]\n#   padL = getMax(y)\n#   y = torch.tensor([padSeq(i,getMax(y),str2int['<End>']) for i in y], dtype=torch.long, device=device)\n#   # y = torch.stack([data[i + 1: i + blk_size + 1] for i in ix]).to(device)\n#   return x, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxC = 0\nmaxI = 0\nfor i in range(len(sentText)):\n    count = 0\n    for w in sentText[i].split():\n        count += 1\n    if count > maxC:\n        maxC = count\n        maxI = i\nmaxC , maxI","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tensor(15.1181) => tensor(14.2519)\ncounts = []\nbigS = []\nfor i in range(len(sentText)):\n    count = 0\n    for w in sentText[i].split():\n        count += 1\n    counts.append(count)\n    if count > 64:\n        bigS.append(i)\ncounts = torch.tensor(counts,dtype=torch.float32)\ncounts.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentText","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\ndef clean_text(text):\n    # Remove unwanted characters and symbols\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    text = re.sub(r'\\.\\s*', '. ', text)  # Ensure periods are followed by a single space\n    text = re.sub(r'\\s*\\.\\s*', '. ', text)  # Remove spaces before periods\n    return text.strip()\n\n# Function to tokenize sentences using SpaCy\ndef spacyST(text):\n    doc = nlp(clean_text(text))\n    return [sent.text for sent in doc.sents]\nsentText = spacyST(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padL(st, l):\n    L1 = st[:]\n    diff = l - len(st)\n    if diff != 0:\n        for i in range(diff):\n            L1+=\"<EOS>\"\n    return L1\n\n\n\nresults = collection.query(\n    query_texts=padL(\"milestone model architectures\",64) # Chroma will embed this for you\n#     n_results=2 # how many results to return\n)\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Example query function\n# def query_chroma(query_text):\n#     query_embedding = get_embeddings(query_text)\n#     results = collection.query(embedding=query_embedding, n_results=5)\n#     return results\n\n# # Example usage\n# query_results = query_chroma(\"milestone model architectures\")\n# for result in query_results['results']:\n#     print(result['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}